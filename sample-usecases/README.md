# LLM TaskBench - Use Case Collection

A collection of evaluation use cases for benchmarking LLM reasoning and text processing capabilities.

## Use Cases

| # | Use Case | Difficulty | Primary Capability | Files | Results |
|---|----------|------------|-------------------|-------|---------|
| 00 | [Lecture Concept Extraction](00-lecture-concept-extraction/) | Moderate-Hard | Reasoning + Structured Extraction | 3 lectures | [ğŸ“Š Results](00-lecture-concept-extraction/taskbench-results.md) |
| 01 | [Meeting Notes â†’ Action Items](01-meeting-action-items/) | Moderate | Extraction + Inference | 4 transcripts | [ğŸ“Š Results](01-meeting-action-items/taskbench-results.md) |
| 02 | [Bug Report Triage](02-bug-report-triage/) | Moderate-Hard | Classification + Reasoning | 25 reports | [ğŸ“Š Results](02-bug-report-triage/taskbench-results.md) |
| 03 | [Regex Generation](03-regex-generation/) | Hard | Pattern Recognition + Logic | 10 challenges | [ğŸ“Š Results](03-regex-generation/taskbench-results.md) |
| 04 | [Data Cleaning Rules](04-data-cleaning-rules/) | Moderate-Hard | Pattern Recognition + Structured Output | 1 dataset | [ğŸ“Š Results](04-data-cleaning-rules/taskbench-results.md) |

## Benchmark Summary

Latest results comparing **Claude Sonnet 4** vs **GPT-4o-mini**:

| Use Case | Winner | Winner Score | Runner-up Score | Key Insight |
|----------|--------|--------------|-----------------|-------------|
| 00 - Lecture Concepts | Claude Sonnet 4 | 93/100 | 35/100 | GPT-4o-mini over-segments, ignoring duration constraints |
| 01 - Meeting Actions | Claude Sonnet 4 | 82/100 | 66/100 | GPT-4o-mini misses implicit commitments (44% recall) |
| 02 - Bug Triage | Claude Sonnet 4 | 86/100 | 75/100 | Both usable; Claude has fewer misclassifications |
| 03 - Regex Generation | Claude Sonnet 4 | 97/100 | 0/100 | GPT-4o-mini fails entirely on logical reasoning tasks |
| 04 - Data Cleaning | Claude Sonnet 4 | 88/100 | 76/100 | Both usable; Claude generates more complete rules |

**Overall:** Claude Sonnet 4 outperforms GPT-4o-mini across all use cases, with the gap being most significant on tasks requiring constraint following (00) and logical reasoning (03).

## Folder Structure

```
llm-taskbench/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ 00-lecture-concept-extraction/
â”‚   â”œâ”€â”€ USE-CASE.md                    # Use case description & evaluation notes
â”‚   â”œâ”€â”€ data/                          # Input data
â”‚   â”‚   â”œâ”€â”€ lecture-01-python-basics.txt
â”‚   â”‚   â”œâ”€â”€ lecture-02-ml-fundamentals.txt
â”‚   â”‚   â””â”€â”€ lecture-03-system-design.txt
â”‚   â””â”€â”€ ground-truth/                  # Expected outputs
â”‚       â”œâ”€â”€ lecture-01-concepts.csv
â”‚       â”œâ”€â”€ lecture-01-metadata.json
â”‚       â”œâ”€â”€ lecture-02-concepts.csv
â”‚       â”œâ”€â”€ lecture-02-metadata.json
â”‚       â”œâ”€â”€ lecture-03-concepts.csv
â”‚       â””â”€â”€ lecture-03-metadata.json
â”œâ”€â”€ 01-meeting-action-items/
â”‚   â”œâ”€â”€ USE-CASE.md                    # Use case description & evaluation notes
â”‚   â”œâ”€â”€ data/                          # Input data
â”‚   â”‚   â”œâ”€â”€ meeting-01-standup.txt
â”‚   â”‚   â”œâ”€â”€ meeting-02-planning.txt
â”‚   â”‚   â”œâ”€â”€ meeting-03-incident.txt
â”‚   â”‚   â””â”€â”€ meeting-04-strategy.txt
â”‚   â””â”€â”€ ground-truth/                  # Expected outputs
â”‚       â”œâ”€â”€ meeting-01-standup.json
â”‚       â”œâ”€â”€ meeting-02-planning.json
â”‚       â”œâ”€â”€ meeting-03-incident.json
â”‚       â””â”€â”€ meeting-04-strategy.json
â”œâ”€â”€ 02-bug-report-triage/
â”‚   â”œâ”€â”€ USE-CASE.md
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ bug-reports.csv
â”‚   â””â”€â”€ ground-truth/
â”‚       â””â”€â”€ bug-reports-labeled.json
â”œâ”€â”€ 03-regex-generation/
â”‚   â”œâ”€â”€ USE-CASE.md
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ regex-challenges.json
â”‚   â””â”€â”€ ground-truth/
â”‚       â””â”€â”€ regex-solutions.json
â””â”€â”€ 04-data-cleaning-rules/
    â”œâ”€â”€ USE-CASE.md
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ customers-messy.csv
    â””â”€â”€ ground-truth/
        â”œâ”€â”€ customers-issues.json
        â””â”€â”€ customers-clean.csv
```

## How to Use

### For Each Use Case:

1. **Read the USE-CASE.md** - Contains:
   - Goal and context
   - Expected output schema
   - Evaluation metrics
   - LLM notes (what this tests)

2. **Examine the data/** folder - Input files for the LLM

3. **Compare against ground-truth/** - Expected outputs for scoring

### Evaluation Flow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USE-CASE.md     â”‚â”€â”€â”€â”€â–¶â”‚ Prompt + Data   â”‚â”€â”€â”€â”€â–¶â”‚ LLM Response    â”‚
â”‚ (read first)    â”‚     â”‚ (to LLM)        â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                        â”‚ Ground Truth    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ (compare)       â”‚    Evaluate
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Use Case Details

### 00 - Lecture Concept Extraction

**Input:** Lecture transcript with timestamps (text)
**Output:** CSV with concept name, start time, end time

**What it tests:**
- Understanding teaching flow and pedagogical structure
- Precise timestamp boundary identification
- Rule following (2-7 minute duration constraints)
- Handling breaks, Q&A, tangents

**Difficulty progression:**
- lecture-01: Easy (linear structure, clear transitions)
- lecture-02: Medium (includes 5-min break, more complex topics)
- lecture-03: Hard (tangents, inline Q&A, stories, break)

---

### 01 - Meeting Notes Action Item Extraction

**Input:** Meeting transcript (text)
**Output:** Structured JSON with action items, owners, deadlines, decisions

**What it tests:**
- Inference (implicit deadlines like "by end of sprint")
- Attribution (who owns ambiguous tasks)
- Distinguishing discussion from commitments

**Difficulty progression:**
- meeting-01: Easy (explicit items, clear owners)
- meeting-02: Medium (dependencies, conditional items)
- meeting-03: Hard (urgent items, unclear ownership)
- meeting-04: Hard (vague timelines, strategic decisions)

---

### 02 - Bug Report Triage Classification

**Input:** Bug report (title + description)
**Output:** Severity, component, type, root cause, duplicates

**What it tests:**
- Classification consistency
- Handling vague reports
- Duplicate detection across phrasings
- Distinguishing bugs from feature requests

---

### 03 - Regex Pattern Generation

**Input:** Positive examples (must match) + Negative examples (must not match)
**Output:** Working regex pattern

**What it tests:**
- Pure logical reasoning
- Pattern recognition
- Generalization without overfitting
- Binary pass/fail validation

---

### 04 - Data Cleaning Rule Generation

**Input:** Messy CSV with quality issues
**Output:** Actionable cleaning rules

**What it tests:**
- Pattern recognition in noisy data
- Generating executable (not vague) transformations
- Prioritization of issues
- Handling ambiguous cases

---

## Scoring Guidelines

Each use case has specific metrics in its USE-CASE.md. General quality tiers:

| Rating | Criteria |
|--------|----------|
| Excellent | Matches ground truth 90%+, handles edge cases |
| Good | Matches 75-90%, minor misses on edge cases |
| Acceptable | Matches 60-75%, usable with cleanup |
| Poor | Below 60%, significant errors |

## Adding New Use Cases

1. Create folder: `XX-use-case-name/`
2. Add `USE-CASE.md` with standard sections
3. Add `data/` folder with input files
4. Add `ground-truth/` folder with expected outputs
5. Update this README
