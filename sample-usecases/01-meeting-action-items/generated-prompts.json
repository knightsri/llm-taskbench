{
  "analysis": {
    "transformation_type": "Structured information extraction from unstructured conversational text with inference and attribution",
    "key_fields": [
      "action_items (with owner, deadline, priority, dependencies, status)",
      "owner_confidence (explicit|inferred|unclear)",
      "deadline_type (explicit|relative|inferred|none)",
      "decisions",
      "open_questions",
      "context (supporting quotes from transcript)"
    ],
    "quality_indicators": [
      "Accurate extraction of all action items without hallucination",
      "Correct owner attribution with appropriate confidence levels",
      "Proper deadline extraction and classification",
      "Inclusion of supporting context from transcript",
      "Identification of dependencies between tasks",
      "Correct status inference from conversation",
      "Capture of decisions and open questions",
      "Proper handling of implicit commitments"
    ],
    "comparison_strategy": "Field-by-field comparison with ground truth: (1) Check for missing action items (recall), (2) Check for hallucinated items (precision), (3) Verify owner accuracy and confidence levels, (4) Validate deadline extraction and type classification, (5) Confirm dependency relationships, (6) Assess context quote relevance, (7) Evaluate decisions and open questions completeness"
  },
  "task_prompt": "You are an AI assistant that extracts structured action items, decisions, and open questions from meeting transcripts.\n\n# TASK\nAnalyze the provided meeting transcript and extract:\n1. **Action Items**: Tasks assigned or committed to during the meeting\n2. **Decisions Made**: Key decisions reached\n3. **Open Questions**: Unresolved items needing follow-up\n\n# INSTRUCTIONS\n\n## Action Items\nFor each action item, identify:\n- **action**: Clear description of what needs to be done\n- **owner**: Person responsible (extract from explicit assignments or infer from context)\n- **owner_confidence**: \n  - \"explicit\" if directly assigned (\"Sarah, can you...\", \"I will...\")\n  - \"inferred\" if implied by context (\"someone from engineering\", role-based)\n  - \"unclear\" if ambiguous\n- **deadline**: Date in YYYY-MM-DD format (calculate from meeting date if relative)\n- **deadline_type**:\n  - \"explicit\" if specific date/time mentioned\n  - \"relative\" if time-based (\"this morning\", \"by end of day\")\n  - \"inferred\" if implied by context (\"today\" from standup context)\n  - \"none\" if no deadline mentioned\n- **deadline_raw**: Original text from transcript\n- **priority**: Assess as critical/high/medium/low based on urgency and context\n- **dependencies**: IDs of other action items this depends on\n- **context**: Direct quote or paraphrase from transcript supporting this action item\n- **status**: \n  - \"new\" for newly assigned tasks\n  - \"in-progress\" if actively being worked on\n  - \"blocked\" if waiting on something\n  - \"done\" if completed\n\n## Decisions\nExtract key decisions with:\n- **decision**: What was decided\n- **context**: Brief background or reasoning\n- **stakeholders**: People involved in the decision\n\n## Open Questions\nCapture unresolved items with:\n- **question**: The unresolved item\n- **raised_by**: Person who raised it\n- **assigned_to**: Person assigned to follow up (or null)\n\n# IMPORTANT GUIDELINES\n- Do NOT hallucinate action items not present in the transcript\n- Capture implicit commitments (\"I'll...\", \"I should...\", \"I need to...\")\n- Pay attention to conditional tasks and note them appropriately\n- If a task is mentioned then cancelled/changed, only include the final version\n- For delegated tasks (\"Sarah, ask Mike to...\"), the owner is the person being asked (Sarah)\n- Calculate relative deadlines from the meeting date provided in the transcript\n- Include supporting context quotes to justify each extraction\n\n# OUTPUT FORMAT\nProvide your response as valid JSON matching this schema:\n\n```json\n{\n  \"meeting_id\": \"string\",\n  \"meeting_date\": \"YYYY-MM-DD\",\n  \"action_items\": [\n    {\n      \"id\": \"AI-001\",\n      \"action\": \"Description of what needs to be done\",\n      \"owner\": \"Person Name\",\n      \"owner_confidence\": \"explicit|inferred|unclear\",\n      \"deadline\": \"YYYY-MM-DD\",\n      \"deadline_type\": \"explicit|relative|inferred|none\",\n      \"deadline_raw\": \"original text from transcript\",\n      \"priority\": \"critical|high|medium|low\",\n      \"dependencies\": [\"AI-002\"],\n      \"context\": \"Quote from transcript\",\n      \"status\": \"new|in-progress|blocked|done\"\n    }\n  ],\n  \"decisions\": [\n    {\n      \"decision\": \"What was decided\",\n      \"context\": \"Brief background\",\n      \"stakeholders\": [\"Person1\", \"Person2\"]\n    }\n  ],\n  \"open_questions\": [\n    {\n      \"question\": \"Unresolved item\",\n      \"raised_by\": \"Person Name\",\n      \"assigned_to\": \"Person Name or null\"\n    }\n  ]\n}\n```\n\nNow analyze this meeting transcript:\n\n{INPUT_TRANSCRIPT}",
  "judge_prompt": "You are an expert evaluator assessing the quality of action item extraction from meeting transcripts.\n\n# YOUR TASK\nCompare the MODEL OUTPUT against the GROUND TRUTH and evaluate based on:\n1. **Recall**: Did the model capture all action items present in ground truth?\n2. **Precision**: Did the model avoid hallucinating action items not in ground truth?\n3. **Attribution Accuracy**: Are owners correctly identified with appropriate confidence levels?\n4. **Deadline Accuracy**: Are deadlines correctly extracted and classified?\n5. **Dependency Accuracy**: Are task dependencies correctly identified?\n6. **Context Quality**: Are supporting quotes relevant and accurate?\n7. **Completeness**: Are decisions and open questions captured?\n\n# EVALUATION CRITERIA\n\n## CRITICAL VIOLATIONS (8 points each)\n- **Missing Action Item**: Ground truth contains an action item completely absent from model output\n- **Hallucinated Action Item**: Model output contains an action item not present in ground truth\n- **Wrong Owner**: Owner assigned to action item differs from ground truth\n- **Invalid JSON**: Output is not valid, parseable JSON\n- **Schema Violation**: Output does not match required schema structure\n\n## HIGH SEVERITY (5 points each)\n- **Wrong Owner Confidence**: owner_confidence classification differs from ground truth (explicit vs inferred vs unclear)\n- **Wrong Deadline Date**: Calculated deadline date differs from ground truth by more than 1 day\n- **Wrong Deadline Type**: deadline_type classification differs from ground truth\n- **Missing Dependency**: Ground truth shows dependency not captured in model output\n- **Wrong Status**: Status classification differs from ground truth\n- **Wrong Priority**: Priority differs by more than one level from ground truth\n\n## MEDIUM SEVERITY (3 points each)\n- **Missing Decision**: Ground truth contains a decision not captured in model output\n- **Missing Open Question**: Ground truth contains an open question not captured in model output\n- **Poor Context**: Context quote is missing, irrelevant, or significantly inaccurate\n- **Incomplete Action Description**: Action description is vague or missing key details present in ground truth\n- **Wrong Priority (minor)**: Priority differs by exactly one level from ground truth\n\n## LOW SEVERITY (1 point each)\n- **Minor Formatting Issue**: Small inconsistencies in formatting (capitalization, punctuation)\n- **ID Mismatch**: Action item IDs don't match ground truth format (but are consistent)\n- **Deadline Raw Text**: deadline_raw differs slightly from ground truth but conveys same meaning\n\n# SCORING METHODOLOGY\n\n## Step 1: Calculate Violation Penalties\n- Sum all penalty points from violations detected\n- Maximum possible penalties: 200 points\n\n## Step 2: Calculate Component Scores\n\n### Accuracy Score (40% weight)\n- **Recall**: (Action items in output that match ground truth) / (Total ground truth action items) \u00d7 100\n- **Precision**: (Action items in output that match ground truth) / (Total output action items) \u00d7 100\n- **Accuracy Score** = (Recall + Precision) / 2\n\n### Format Score (20% weight)\n- Start at 100\n- Deduct for JSON validity issues, schema violations, formatting problems\n- Minimum: 0\n\n### Compliance Score (40% weight)\n- Start at 100\n- Subtract violation penalties\n- Minimum: 0\n\n## Step 3: Calculate Final Score\nFinal Score = (Accuracy Score \u00d7 0.4) + (Format Score \u00d7 0.2) + (Compliance Score \u00d7 0.4)\n\n# OUTPUT FORMAT\n\nProvide your evaluation as JSON:\n\n```json\n{\n  \"violations\": [\n    {\n      \"type\": \"violation_name\",\n      \"severity\": \"CRITICAL|HIGH|MEDIUM|LOW\",\n      \"description\": \"Detailed explanation\",\n      \"ground_truth_value\": \"expected value\",\n      \"model_output_value\": \"actual value\",\n      \"penalty\": 8\n    }\n  ],\n  \"metrics\": {\n    \"recall\": 85.7,\n    \"precision\": 100.0,\n    \"accuracy_score\": 92.85,\n    \"format_score\": 100.0,\n    \"compliance_score\": 76.0,\n    \"total_penalties\": 24\n  },\n  \"final_score\": 87.14,\n  \"summary\": \"Brief assessment of output quality\",\n  \"strengths\": [\"What the model did well\"],\n  \"weaknesses\": [\"What the model missed or got wrong\"]\n}\n```\n\n# INPUTS\n\n## GROUND TRUTH\n{GROUND_TRUTH}\n\n## MODEL OUTPUT\n{MODEL_OUTPUT}\n\nProvide your evaluation now.",
  "rubric": {
    "critical_requirements": [
      {
        "name": "Complete Action Item Extraction",
        "description": "All action items present in ground truth must be extracted without omissions",
        "derived_from": "Ground truth contains 7 action items (AI-001 through AI-007) representing all commitments made during the meeting",
        "penalty_per_violation": 8
      },
      {
        "name": "No Hallucinated Action Items",
        "description": "Model must not invent action items not present in the transcript or ground truth",
        "derived_from": "Precision is a key metric; ground truth only includes items explicitly or implicitly committed to in the transcript",
        "penalty_per_violation": 8
      },
      {
        "name": "Correct Owner Attribution",
        "description": "Each action item must be assigned to the correct person responsible",
        "derived_from": "Ground truth shows specific owner assignments (Sarah, Mike, Priya, Tom, Lisa) based on explicit statements or clear inference",
        "penalty_per_violation": 8
      },
      {
        "name": "Valid JSON Output",
        "description": "Output must be valid, parseable JSON",
        "derived_from": "Expected output format is JSON; invalid JSON cannot be processed by project management tools",
        "penalty_per_violation": 8
      },
      {
        "name": "Schema Compliance",
        "description": "Output must match the required schema structure with all mandatory fields",
        "derived_from": "Ground truth follows specific schema with meeting_id, action_items array, decisions array, open_questions array",
        "penalty_per_violation": 8
      }
    ],
    "compliance_checks": [
      {
        "check": "Owner Confidence Classification",
        "condition": "owner_confidence value differs from ground truth (explicit vs inferred vs unclear)",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Deadline Date Accuracy",
        "condition": "Calculated deadline date differs from ground truth by more than 1 day",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Deadline Type Classification",
        "condition": "deadline_type differs from ground truth (explicit vs relative vs inferred vs none)",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Dependency Identification",
        "condition": "Dependencies array missing items present in ground truth (e.g., AI-005 depends on AI-004)",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Status Classification",
        "condition": "Status differs from ground truth (new vs in-progress vs blocked vs done)",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Priority Assessment Major",
        "condition": "Priority differs by more than one level from ground truth",
        "severity": "HIGH",
        "penalty": 5
      },
      {
        "check": "Decision Extraction",
        "condition": "Decisions present in ground truth are missing from model output",
        "severity": "MEDIUM",
        "penalty": 3
      },
      {
        "check": "Open Question Extraction",
        "condition": "Open questions present in ground truth are missing from model output",
        "severity": "MEDIUM",
        "penalty": 3
      },
      {
        "check": "Context Quote Quality",
        "condition": "Context field is missing, irrelevant, or significantly inaccurate compared to ground truth",
        "severity": "MEDIUM",
        "penalty": 3
      },
      {
        "check": "Action Description Completeness",
        "condition": "Action description is vague or missing key details present in ground truth",
        "severity": "MEDIUM",
        "penalty": 3
      },
      {
        "check": "Priority Assessment Minor",
        "condition": "Priority differs by exactly one level from ground truth (e.g., high vs medium)",
        "severity": "MEDIUM",
        "penalty": 3
      },
      {
        "check": "Formatting Consistency",
        "condition": "Minor inconsistencies in capitalization, punctuation, or text formatting",
        "severity": "LOW",
        "penalty": 1
      },
      {
        "check": "ID Format",
        "condition": "Action item IDs don't match ground truth format but are internally consistent",
        "severity": "LOW",
        "penalty": 1
      },
      {
        "check": "Deadline Raw Text",
        "condition": "deadline_raw differs slightly from ground truth but conveys same meaning",
        "severity": "LOW",
        "penalty": 1
      }
    ],
    "weights": {
      "accuracy": 40,
      "format": 20,
      "compliance": 40
    },
    "scoring_instructions": "Calculate final score as weighted average: (1) Accuracy Score (40%): Average of recall and precision percentages where recall = matched items / ground truth items and precision = matched items / output items. (2) Format Score (20%): Start at 100, deduct for JSON and schema violations. (3) Compliance Score (40%): Start at 100, subtract total violation penalties (max 200 possible). Final Score = (Accuracy \u00d7 0.4) + (Format \u00d7 0.2) + (Compliance \u00d7 0.4). Score range: 0-100."
  }
}