# LLM TaskBench ğŸš€

**Task-specific LLM evaluation framework with agentic orchestration and LLM-as-judge evaluation**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ğŸ“– Overview

LLM TaskBench shifts from **metric-first** to **task-first** LLM evaluation. Instead of relying on generic metrics like BLEU or ROUGE, evaluate models on **your actual use cases** with task-specific criteria.

### Why LLM TaskBench?

Traditional LLM benchmarks don't tell you which model is best for **your** task. Our research on 42 production LLMs revealed:

- âŒ **Model size doesn't correlate with quality** - 405B models didn't beat 72B models
- âŒ **"Reasoning" models can underperform** on reasoning tasks
- âŒ **Cost has zero correlation** with performance
- âœ… **Fine-tuning beats raw parameter count**

**LLM TaskBench** lets you discover these insights for your specific use case.

### Key Features

- ğŸ¯ **Task-First Evaluation** - Define your own evaluation tasks with custom criteria
- ğŸ¤– **LLM-as-Judge** - Automated quality assessment using Claude Sonnet 4.5
- ğŸ’° **Cost-Aware Recommendations** - Find the best value model for your budget
- ğŸ”„ **Multi-Model Comparison** - Evaluate 5+ models simultaneously
- ğŸ“Š **Detailed Analytics** - Scores, violations, token usage, and cost breakdowns
- ğŸš€ **Production Ready** - Retry logic, rate limiting, comprehensive error handling

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/knightsri/llm-taskbench.git
cd llm-taskbench

# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .

# Set up your API key
cp .env.example .env
# Edit .env and add your OPENROUTER_API_KEY
```

### Run Your First Evaluation

```bash
# Evaluate 3 models on lecture concept extraction
taskbench evaluate tasks/lecture_analysis.yaml \
  --models anthropic/claude-sonnet-4.5,openai/gpt-4o,qwen/qwen-2.5-72b-instruct \
  --input-file tests/fixtures/sample_transcript.txt
```

**Output:**
```
Evaluating 3 models on task 'lecture_concept_extraction'

âœ“ anthropic/claude-sonnet-4.5: 15,234 tokens, $0.36, 2,145ms
âœ“ openai/gpt-4o: 16,012 tokens, $0.42, 1,876ms
âœ“ qwen/qwen-2.5-72b-instruct: 14,567 tokens, $0.18, 3,201ms

Running LLM-as-judge evaluation...

âœ“ anthropic/claude-sonnet-4.5: Score 98/100, 0 violations
âœ“ openai/gpt-4o: Score 95/100, 1 violations
âœ“ qwen/qwen-2.5-72b-instruct: Score 87/100, 3 violations

                      Model Comparison Results
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rank â”‚ Model              â”‚ Score â”‚ Violations â”‚ Cost     â”‚ Tokens â”‚ Value â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1    â”‚ claude-sonnet-4.5  â”‚  98   â”‚     0      â”‚ $0.3600  â”‚ 15,234 â”‚ â­â­â­ â”‚
â”‚ 2    â”‚ gpt-4o             â”‚  95   â”‚     1      â”‚ $0.4200  â”‚ 16,012 â”‚ â­â­   â”‚
â”‚ 3    â”‚ qwen-2.5-72b       â”‚  87   â”‚     3      â”‚ $0.1800  â”‚ 14,567 â”‚ â­â­â­ â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š RECOMMENDATIONS

ğŸ† Best Overall: anthropic/claude-sonnet-4.5
   Score: 98/100, Cost: $0.3600

ğŸ’ Best Value: qwen/qwen-2.5-72b-instruct
   Score: 87/100, Cost: $0.1800

âœ“ Results saved to results/evaluation_results.json
```

---

## ğŸ“š Documentation

- **[Architecture Guide](docs/ARCHITECTURE.md)** - System design and component overview
- **[API Reference](docs/API.md)** - Complete API documentation
- **[Usage Guide](docs/USAGE.md)** - Detailed tutorials and examples

---

## ğŸ¯ Core Concepts

### 1. Task Definitions

Define evaluation tasks using YAML:

```yaml
name: "lecture_concept_extraction"
description: "Extract teaching concepts from lecture transcripts with precise timestamps"
input_type: "transcript"
output_format: "csv"

evaluation_criteria:
  - "Timestamp accuracy (within Â±5 seconds)"
  - "Duration compliance (2-7 minutes per segment)"
  - "Concept names are descriptive and clear"

constraints:
  min_duration_minutes: 2
  max_duration_minutes: 7
  required_csv_columns: ["concept", "start_time", "end_time"]

judge_instructions: |
  Evaluate the model's output based on:
  1. Accuracy (40%): Are concepts correctly identified?
  2. Format (30%): Valid CSV with required columns?
  3. Compliance (30%): Meet duration constraints?
```

### 2. LLM-as-Judge Evaluation

Automatically evaluate outputs using Claude Sonnet 4.5:

- **Accuracy Score** (0-100): Content quality
- **Format Score** (0-100): Structure compliance
- **Compliance Score** (0-100): Constraint adherence
- **Violations**: Specific issues found

### 3. Cost-Aware Recommendations

Get actionable recommendations based on:

- **Best Overall**: Highest quality (98/100)
- **Best Value**: Best score/cost ratio (87/100 for 50% less)
- **Budget Option**: Acceptable quality at lowest cost

---

## ğŸ’» CLI Commands

### Evaluate Models

```bash
taskbench evaluate <task.yaml> --models <model-list> --input-file <input.txt>
```

### List Available Models

```bash
taskbench models --list
```

### Validate Task Definition

```bash
taskbench validate <task.yaml>
```

---

## ğŸ—ï¸ Project Structure

```
llm-taskbench/
â”œâ”€â”€ src/taskbench/
â”‚   â”œâ”€â”€ core/           # Data models and task parsing
â”‚   â”œâ”€â”€ api/            # OpenRouter API client
â”‚   â”œâ”€â”€ evaluation/     # Executor, judge, cost tracking
â”‚   â””â”€â”€ cli/            # Command-line interface
â”œâ”€â”€ tasks/              # Built-in task definitions
â”œâ”€â”€ tests/              # Test suite
â”œâ”€â”€ docs/               # Documentation
â””â”€â”€ config/             # Model pricing database
```

---

## ğŸ”¬ Research Background

Based on evaluating **42 production LLMs** on lecture analysis:

| Finding | Impact |
|---------|--------|
| Model size â‰  quality | 72B models beat 405B models |
| "Reasoning" â‰  better reasoning | Some reasoning models scored lower |
| Cost â‰  quality | Zero correlation found |
| Fine-tuning > parameters | Specialized models outperform larger general models |

**Conclusion**: You need task-specific evaluation to find the right model.

---

## ğŸ“ Use Cases

### 1. Lecture Transcript Analysis
Extract teaching concepts with timestamps - perfect for educational platforms.

### 2. Customer Support Classification
Evaluate models on classifying support tickets with your categories.

### 3. Code Generation
Test models on generating code for your specific framework/library.

### 4. Content Moderation
Compare models on detecting violations according to your guidelines.

### 5. Custom NLP Tasks
Any task where generic benchmarks don't tell the full story.

---

## ğŸ› ï¸ Development

### Run Tests

```bash
# All tests
pytest

# With coverage
pytest --cov=taskbench --cov-report=html

# Specific module
pytest tests/test_models.py -v
```

### Code Quality

```bash
# Format code
black src/ tests/
isort src/ tests/

# Type checking
mypy src/

# Linting
flake8 src/
```

---

## ğŸ—ºï¸ Roadmap

### Phase 1: MVP (Current)
- âœ… Core framework
- âœ… LLM-as-judge evaluation
- âœ… CLI interface
- âœ… Cost tracking

### Phase 2: Enhanced Features
- â³ Batch evaluation
- â³ Custom judge models
- â³ Results visualization
- â³ Web interface

### Phase 3: Advanced Analytics
- ğŸ“‹ Historical tracking
- ğŸ“‹ A/B testing
- ğŸ“‹ Regression detection
- ğŸ“‹ Fine-tuning guidance

---

## ğŸ¤ Contributing

Contributions welcome! Please see our contributing guidelines (coming soon).

### Ways to Contribute

- ğŸ› Report bugs
- ğŸ’¡ Suggest features
- ğŸ“ Improve documentation
- ğŸ”§ Submit pull requests
- ğŸ“Š Share your task definitions

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **OpenRouter** for unified LLM API access
- **Anthropic** for Claude Sonnet 4.5
- **Research participants** who tested 42 models

---

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/knightsri/llm-taskbench/issues)
- **Discussions**: [GitHub Discussions](https://github.com/knightsri/llm-taskbench/discussions)

---

## â­ Star History

If you find LLM TaskBench useful, please consider giving it a star! â­

---

**Built with â¤ï¸ for developers who need real-world LLM evaluation**
