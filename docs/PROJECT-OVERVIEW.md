# LLM TaskBench - Project Overview

**Version:** 2.0 (MVP)  
**Author:** Sri Bolisetty (@KnightSri)  
**Last Updated:** November 2025  
**Timeline:** 6-8 weeks  
**Target Completion:** December 22, 2025  
**Demo Day:** January 17, 2026

---

## What is LLM TaskBench?

LLM TaskBench is a **task-first LLM evaluation framework** that enables domain experts to compare multiple LLMs on their actual use cases without requiring AI engineering expertise.

**The Problem It Solves:**

Existing LLM evaluation tools (DeepEval, Promptfoo, Eleuther AI) focus on generic benchmarks (BLEU, ROUGE) and are built for AI engineers, not domain experts. When a medical professional needs to know "which LLM best triages patient symptoms," or an educator wants "the most cost-effective model for lecture summarization," current tools provide unhelpful generic scores.

**The Solution:**

LLM TaskBench shifts evaluation from **metric-first to task-first**:
- Define YOUR actual task in YAML
- Evaluate 5+ models automatically  
- Get cost-aware recommendations

---

## Key Innovation

**Research-Validated Approach:** Based on comprehensive testing of 42 production LLMs on lecture transcript analysis, revealing surprising insights:

âŒ **Conventional Wisdom is Wrong:**
- Model size doesn't correlate with quality (405B didn't beat 72B)
- "Reasoning-optimized" models can perform WORSE on reasoning tasks  
- Cost shows ZERO correlation with performance (36X price range)

âœ… **What Actually Matters:**
- Fine-tuning proves more impactful than raw model size
- Mid-tier models often hit the quality/cost "sweet spot"
- Task-specific evaluation reveals model strengths/weaknesses

**Read the full research:** ["How a $3.45 API Call Taught Me Everything About LLM Cost Optimization"](https://www.linkedin.com/pulse/how-345-api-call-taught-me-everything-llm-cost-sri-bolisetty-vxate)

---

## Primary Use Case

**Lecture Transcript Concept Extraction**

Extract teaching concepts from lecture transcripts with:
- Precise timestamps (HH:MM:SS format)
- Duration constraints (2-7 minute segments)
- Descriptive concept names
- CSV output format

**Why This Task?**

It requires:
- Complex reasoning (understanding teaching flow)
- Precise text processing (accurate timestamps)
- Rule following (duration constraints)
- Structured output (clean CSV)

This combination reveals model capabilities beyond simple Q&A.

**Research Baseline (Claude Sonnet 4.5):**
- 24 concepts extracted from 3-hour lecture
- 0 violations (all segments 2-7 minutes)
- $0.36 per transcript
- 100% accuracy benchmark

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLM TaskBench System                   â”‚
â”‚                                                           â”‚
â”‚  User (CLI) â†’ Task Parser â†’ LLM Orchestrator (Agentic)  â”‚
â”‚                                    â†“                      â”‚
â”‚                         Model Execution                   â”‚
â”‚                    (OpenRouter/Direct APIs)               â”‚
â”‚                                    â†“                      â”‚
â”‚                     LLM-as-Judge Evaluator               â”‚
â”‚                    (Claude/GPT-4 scoring)                 â”‚
â”‚                                    â†“                      â”‚
â”‚                    Cost Analysis Engine                   â”‚
â”‚                (Tradeoffs + Recommendations)              â”‚
â”‚                                    â†“                      â”‚
â”‚                    Results + Export                       â”‚
â”‚                  (Tables, CSV, JSON)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Core Components:**

1. **Task Definition System** - Parse & validate YAML task specs
2. **Agentic Orchestrator** - LLM-driven evaluation planning
3. **LLM-as-Judge** - Meta-evaluation with structured scoring
4. **Cost Analyzer** - Real-time tracking + quality tradeoffs
5. **Results Presenter** - Professional output & export

---

## Technology Stack

- **Language:** Python 3.11+
- **CLI:** Typer (commands) + Rich (beautiful terminal output)
- **AI Gateway:** OpenRouter (30+ models, unified API)
- **Data Validation:** Pydantic (type safety)
- **HTTP Client:** httpx (async API calls)
- **Testing:** pytest (80%+ coverage target)
- **Type Safety:** mypy + Pydantic validation

---

## MVP Scope

**Must Have (Non-negotiable):**
- âœ… 1 built-in task (lecture analysis)
- âœ… 5 model evaluation capability
- âœ… LLM-as-judge scoring
- âœ… Cost tracking (accurate to $0.01)
- âœ… Basic CLI interface
- âœ… Basic recommendations
- âœ… 80% test coverage
- âœ… Demo video or live demo

**Can Drop if Timeline is Tight:**
- âŒ Second built-in task
- âŒ Multiple export formats (CSV is enough)
- âŒ Parallel execution (sequential is fine)
- âŒ Advanced CLI features
- âŒ PyPI publication
- âŒ Extensive documentation

---

## Success Metrics

**MVP Requirements:**

| Category | Metric | Target |
|----------|--------|--------|
| **Functionality** | Evaluation speed | 5+ models in <30 min |
| | Accuracy | Judge scores within Â±10 points |
| | Cost tracking | Accurate to $0.01 |
| | Test coverage | 80%+ |
| **Quality** | Research validation | Align with 42-model study |
| | Recommendations | Actionable, not just scores |
| | User experience | First-time user success |
| | Code quality | Type hints, documented |
| **Portfolio** | Documentation | README + API docs |
| | Demo readiness | Zero critical bugs |
| | Architecture | Explainable in 2 min |
| | Professional | GitHub presence |

---

## Development Timeline

**Phase 0: Setup** âœ… Complete (Oct 20-24, 2025)
- Project structure, documentation, repository setup

**Phase 1: Core Framework** ğŸ”œ Next (Oct 27 - Nov 10, 2025)
- Task definition system
- API client with retry logic
- Cost tracking
- Model executor
- Basic CLI

**Phase 2: Evaluation Engine** â³ Pending (Nov 11 - Nov 24, 2025)
- LLM-as-judge implementation
- Violation detection
- Multi-model comparison

**Phase 3: Analysis & Recs** â³ Pending (Nov 25 - Dec 8, 2025)
- Cost-quality tradeoff analysis
- Recommendation engine
- Tier identification

**Phase 4: Polish & Demo** â³ Pending (Dec 9 - Dec 22, 2025)
- Comprehensive testing
- Documentation
- Demo preparation
- Presentation creation

**Overall Progress:** 10% (Phase 0 complete)

---

## Key Features

### 1. Task-First Evaluation
Define your actual use case, not a proxy metric:
- Lecture concept extraction with timestamps
- Medical case triage
- Support ticket categorization  
- Contract analysis
- *Your domain-specific task*

### 2. LLM-as-Judge Quality Assessment
Let Claude/GPT-4 score outputs on *your* criteria:
- Accuracy scores (0-100)
- Format compliance
- Rule violation detection
- Reasoning for scores

### 3. Cost-Aware Recommendations
Balance quality and budget:
- Real-time token tracking (accurate to $0.01)
- Cost-quality tradeoff analysis
- Production vs. development recommendations
- "Sweet spot" identification

### 4. Research-Validated
Built on findings from testing 42 models:
- Results validated against published research
- No model size assumptions
- Real-world performance data
- Surprising insights documented

### 5. Agentic Architecture
Smart orchestration, not manual coordination:
- LLM creates evaluation plan
- Dynamic model selection
- Intelligent error handling
- Automated comparison

---

## Example Usage

```bash
# Define your task
$ cat lecture_analysis.yaml

# Evaluate multiple models
$ taskbench evaluate lecture_analysis.yaml \
  --models claude-sonnet-4.5,gpt-4o,gemini-2.5-pro,llama-3.1-405b,qwen-2.5-72b

# Results:
Evaluating Claude Sonnet 4.5... âœ“ (24 concepts, 0 violations, $0.36)
Evaluating GPT-4o...           âœ“ (23 concepts, 1 violation, $0.42)
Evaluating Gemini 2.5 Pro...   âœ“ (22 concepts, 2 violations, $0.38)
Evaluating Llama 3.1 405B...   âœ“ (20 concepts, 4 violations, $0.25)
Evaluating Qwen 2.5 72B...     âœ“ (21 concepts, 3 violations, $0.18)

# Get recommendations
$ taskbench recommend

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model               â”‚ Score â”‚ Violations â”‚ Cost     â”‚ Tier      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude Sonnet 4.5   â”‚ 98    â”‚ 0          â”‚ $0.36    â”‚ Excellent â”‚
â”‚ GPT-4o              â”‚ 95    â”‚ 1          â”‚ $0.42    â”‚ Excellent â”‚
â”‚ Gemini 2.5 Pro      â”‚ 92    â”‚ 2          â”‚ $0.38    â”‚ Excellent â”‚
â”‚ Qwen 2.5 72B        â”‚ 87    â”‚ 3          â”‚ $0.18    â”‚ Good      â”‚
â”‚ Llama 3.1 405B      â”‚ 83    â”‚ 4          â”‚ $0.25    â”‚ Good      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š Recommendations:
  ğŸ† Best Overall: Claude Sonnet 4.5
  ğŸ’ Best Value: Qwen 2.5 72B (87% as good, 50% cheaper)
  ğŸ’° Budget Option: Qwen 2.5 72B
```

---

## Project Files Structure

```
llm-taskbench/
â”œâ”€â”€ src/taskbench/           # Main package
â”‚   â”œâ”€â”€ core/                # Task definitions, data models
â”‚   â”œâ”€â”€ api/                 # OpenRouter client, retry logic
â”‚   â”œâ”€â”€ evaluation/          # Executor, judge, cost tracking
â”‚   â”œâ”€â”€ cli/                 # Command-line interface
â”‚   â””â”€â”€ utils/               # Logging, validation
â”œâ”€â”€ tasks/                   # Built-in task definitions
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ examples/                # Example tasks and outputs
â”œâ”€â”€ config/                  # Model pricing database
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ README.md               # Main documentation
```

---

## Critical Path to Success

**Week 1-2 (Phase 1): Foundation**
- MUST complete on time - everything depends on this
- Task parser, API client, cost tracking, basic executor

**Week 3 (Phase 2): Highest Risk**
- LLM-as-judge complexity - start simple
- Get scoring working, refinement can come later

**Week 7: Buffer Week**
- Use if behind schedule
- Catch up on testing or documentation

**Week 8: Polish Only**
- NO new features
- Only bug fixes and documentation
- Demo preparation

---

## Risk Mitigation

**Active Risks:**

| Risk | Mitigation |
|------|------------|
| API rate limits | Cache results, use free tier strategically |
| LLM-as-judge inconsistency | Manual validation, prompt refinement |
| Time overrun Phase 2 | Week 7 buffer, simplified judge for MVP |
| OpenRouter API changes | Abstraction layer, direct API fallbacks |
| Demo day technical issues | Video recording backup, screenshots |

---

## Post-MVP Roadmap

### Immediate (Weeks 9-10)
- Add 3rd built-in task (medical case analysis)
- Improve documentation
- Add Markdown report export
- Package for PyPI

### Phase 2 (Weeks 11+)
- Streamlit web interface
- Batch evaluation (50+ models)
- Historical tracking
- Custom judge LLM selection
- Integration with LangSmith/Helicone

### Community & Growth
- Open source on GitHub (MIT license)
- Blog post announcement
- HackerNews/Reddit posts
- Target: 100 GitHub stars in first month

---

## Demo Readiness Checklist

**Core Functionality:**
- [ ] YAML task parser working
- [ ] LLM orchestrator executes 5 models
- [ ] LLM-as-judge scores all outputs
- [ ] Cost tracking displays real-time costs
- [ ] Results presenter shows comparison table

**Quality Validation:**
- [ ] Judge scores match manual evaluation (Â±10 points)
- [ ] Token counting matches API billing (Â±$0.01)
- [ ] All 5 models complete without errors
- [ ] Results export to JSON and CSV

**Demo Path (10 minutes):**
1. Show task definition (1 min)
2. Run live evaluation (5 min)
3. Show results and recommendations (3 min)
4. Architecture explanation (1 min)

**Backup Plan:**
- Pre-recorded demo video
- Pre-computed results JSON
- Static screenshots

---

## Resources

**Project Documentation:**
- Technical Specification: `technical-spec.md`
- Vision Document: `LLMTaskBench-Vision.md`
- Task List: `claude-code-task-list.md`
- TODO Tracker: `Todo.md`

**Research:**
- Blog Post: ["How a $3.45 API Call Taught Me Everything About LLM Cost Optimization"](https://www.linkedin.com/pulse/how-345-api-call-taught-me-everything-llm-cost-sri-bolisetty-vxate)
- 42-Model Analysis: `analysis.csv`
- Model Data: `models.csv`

**External Tools:**
- OpenRouter: <https://openrouter.ai>
- Anthropic API: <https://docs.anthropic.com>
- OpenAI API: <https://platform.openai.com/docs>

---

## Contact

**Sri Bolisetty**
- GitHub: [@KnightSri](https://github.com/KnightSri)
- LinkedIn: [Profile](https://linkedin.com/in/sribolisetty)
- Project: AI Engineering Capstone

---

**Built with ğŸ§  by Sri Bolisetty**  
*Making LLM evaluation accessible to domain experts*